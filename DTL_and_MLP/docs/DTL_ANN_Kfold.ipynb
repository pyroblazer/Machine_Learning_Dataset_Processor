{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQWm69bMfGY-"
   },
   "source": [
    "Hasil Split & KFold MyMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "wyOWAHx67PHm",
    "outputId": "c9c84885-bc28-4c63-f18f-238f5dfdfac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 90-10\n",
      "Confusion Matrix: \n",
      "[[3 0 0]\n",
      " [0 4 1]\n",
      " [0 0 7]]\n",
      "Accuracy Score: \n",
      "0.9333333333333333\n",
      "Classification Report: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         3\n",
      "Iris-versicolor       1.00      0.80      0.89         5\n",
      " Iris-virginica       0.88      1.00      0.93         7\n",
      "\n",
      "       accuracy                           0.93        15\n",
      "      macro avg       0.96      0.93      0.94        15\n",
      "   weighted avg       0.94      0.93      0.93        15\n",
      "\n",
      "K-Fold\n",
      "Accuracy Score: \n",
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/6 University/Machine Learning/MLP')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from MultilayerPerceptron import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# Kfold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def experiment(mlp, filename):\n",
    "\tmlp.save_model_to_file(filename)\n",
    "\tmlp.load_model_from_file(filename)\n",
    "\n",
    "def split():\n",
    "\tprint(\"Split 90-10\")\n",
    "\n",
    "\tdata = pd.read_csv(\"/content/gdrive/My Drive/6 University/Machine Learning/MLP/iris.csv\")\n",
    "\tinputs = data.drop('species', axis = 1)\n",
    "\ttarget = data['species']\n",
    "\n",
    "\t#training\n",
    "\tdf = pd.concat([inputs, target], axis=1)\n",
    "\ttrain, test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "\ttrainX = train[['sepal_length','sepal_width','petal_length','petal_width']] # taking the training data features\n",
    "\ttrainY = train.species # output of our training data\n",
    "\ttestX = test[['sepal_length','sepal_width','petal_length','petal_width']] # taking test data features\n",
    "\ttestY = test.species   # output value of test data\n",
    "\n",
    "\ttrainX = trainX.reset_index(drop=True)\n",
    "\ttrainY = trainY.reset_index(drop=True)\n",
    "\ttestX = testX.reset_index(drop=True)\n",
    "\ttestY = testY.reset_index(drop=True)\n",
    "\tmlp = myMLP()\n",
    "\tmlp.fit(trainX, trainY)\n",
    "\texperiment(mlp, \"Output-MLP-Split-Test.txt\")\n",
    "\tprediction = mlp.predict(testX)\n",
    "\tconfusion_matrix_results = confusion_matrix(testY.values, prediction)\n",
    "\n",
    "\tprint(\"Confusion Matrix: \")\n",
    "\tprint(confusion_matrix_results)\n",
    "\tprint(\"Accuracy Score: \")\n",
    "\tprint(accuracy_score(testY.values, prediction))\n",
    "\tprint(\"Classification Report: \")\n",
    "\tprint(classification_report(testY.values, prediction))\n",
    "\n",
    "\n",
    "\n",
    "def kfold():\n",
    "\tprint(\"K-Fold\")\n",
    "\n",
    "\t#Training k-fold\n",
    "\tdata = pd.read_csv(\"/content/gdrive/My Drive/6 University/Machine Learning/MLP/iris.csv\")\n",
    "\tinputs = data.drop('species', axis = 1)\n",
    "\ttarget = data['species']\n",
    "\tdf = pd.concat([inputs, target], axis=1)\n",
    "\ttrain, test = train_test_split(df, test_size=0.2)  # divide 150 data to x and y = 150-x\n",
    "\n",
    "\tinputX = train[['sepal_length','sepal_width','petal_length','petal_width']] # taking the training data features\n",
    "\tinputY = train.species # output of our training data\n",
    "\ttestX = test[['sepal_length','sepal_width','petal_length','petal_width']] # taking test data features\n",
    "\ttestY = test.species   # output value of test data\n",
    "\n",
    "\tinputX = inputX.reset_index(drop=True) \n",
    "\tinputY = inputY.reset_index(drop=True)\n",
    "\ttestX = testX.reset_index(drop=True)\n",
    "\ttestY = testY.reset_index(drop=True)\n",
    "\n",
    "\t#k-fold\n",
    "\tkf = KFold(n_splits=10)\n",
    "\tkf.get_n_splits(inputX)\n",
    "\tindices = kf.split(inputX)\n",
    "\n",
    "\tmainmlp = myMLP()\n",
    "\tmaxacc = 0\n",
    "\n",
    "\tfor train_index, test_index in indices: \n",
    "\t\ttrainX, valX = inputX.iloc[train_index], inputX.iloc[test_index]\n",
    "\t\ttrainY, valY = inputY.iloc[train_index], inputY.iloc[test_index]\n",
    "\n",
    "\t\ttrainX = trainX.reset_index(drop=True) \n",
    "\t\ttrainY = trainY.reset_index(drop=True)\n",
    "\t\tvalX = valX.reset_index(drop=True)\n",
    "\t\tvalY = valY.reset_index(drop=True)\n",
    "\n",
    "\t\tmlp = myMLP()\n",
    "\t\tmlp.fit(trainX, trainY)\n",
    "\t\texperiment(mlp, \"Output-MLP-Kfold.txt\")\n",
    "\t\tprediction = mlp.predict(valX)\n",
    "\t\tif (accuracy_score(valY.values, prediction) > maxacc):\n",
    "\t\t\tmainmlp = mlp\n",
    "\t\t\tmaxacc = accuracy_score(valY.values, prediction)\n",
    "\tfinalPrediction = mainmlp.predict(testX)\n",
    "\tprint(\"Accuracy Score: \")\n",
    "\tprint(accuracy_score(testY.values, finalPrediction))\n",
    "\n",
    "\n",
    "split()\n",
    "kfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkNTilDHfCiD"
   },
   "source": [
    "Hasil Split dan Kfold DTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "26GwBqh6fQkf",
    "outputId": "23ff1a7e-676b-4eff-9da1-72020f68e9e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 90-10\n",
      "Confusion Matrix: \n",
      "[[3 0 0]\n",
      " [0 4 5]\n",
      " [0 0 3]]\n",
      "Accuracy Score: \n",
      "0.6666666666666666\n",
      "Classification Report: \n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00         3\n",
      "Iris-versicolor       1.00      0.44      0.62         9\n",
      " Iris-virginica       0.38      1.00      0.55         3\n",
      "\n",
      "       accuracy                           0.67        15\n",
      "      macro avg       0.79      0.81      0.72        15\n",
      "   weighted avg       0.88      0.67      0.68        15\n",
      "\n",
      "K-Fold\n",
      "Accuracy Score: \n",
      "0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from myc45 import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/6 University/Machine Learning/MLP')\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# Kfold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def experiment(c45):\n",
    "    c45.id3.tree_.save_to_file(\"Output-C45.txt\")\n",
    "    c45.id3.tree_.load_from_file(\"Output-C45.txt\")\n",
    "    dict = {\n",
    "        \"sepal_length\": 5.5,\n",
    "        \"sepal_width\": 3.3,\n",
    "        \"petal_length\": 2.0,\n",
    "        \"petal_width\": 0.65\n",
    "    }\n",
    "    print(c45.classify(c45.id3.tree_, dict))\n",
    "\n",
    "def split():\n",
    "    print(\"Split 90-10\")\n",
    "\n",
    "    df = pd.read_csv('/content/gdrive/My Drive/6 University/Machine Learning/MLP/iris.csv', sep=',')\n",
    "    train, test = train_test_split(df, test_size=0.1)\n",
    "\n",
    "    # Main Program Goes Here:\n",
    "    data = pd.read_csv(\"/content/gdrive/My Drive/6 University/Machine Learning/MLP/iris.csv\")\n",
    "\n",
    "    inputs = data.drop('species', axis = 1)\n",
    "    target = data['species']\n",
    "\n",
    "    #training\n",
    "    df = pd.concat([inputs, target], axis=1)\n",
    "    train, test = train_test_split(df, test_size=0.1)\n",
    "    train = train.reset_index(drop=True)\n",
    "\n",
    "    testX = test[['sepal_length','sepal_width','petal_length','petal_width']] # taking test data features\n",
    "    testY = test.species   # output value of test data\n",
    "\n",
    "    testX = testX.reset_index(drop=True)\n",
    "    testY = testY.reset_index(drop=True)\n",
    "\n",
    "    attributes = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    target = 'species'\n",
    "\n",
    "    c45 = myC45(train, target, attributes)\n",
    "    prediction = []\n",
    "    for i in range(len(testX)):\n",
    "        prediction.append(c45.classify(c45.id3.tree_, testX.iloc[i]))\n",
    "    confusion_matrix_results = confusion_matrix(testY.values, prediction)\n",
    "\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(confusion_matrix_results)\n",
    "    print(\"Accuracy Score: \")\n",
    "    print(accuracy_score(testY.values, prediction))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(testY.values, prediction))\n",
    "    # experiment(c45)\n",
    "\n",
    "\n",
    "def kfold():\n",
    "    print(\"K-Fold\")\n",
    "    #Training k-fold\n",
    "    df = pd.read_csv('/content/gdrive/My Drive/6 University/Machine Learning/MLP/iris.csv', sep=',')\n",
    "    data, test = train_test_split(df, test_size=0.2)  # divide 150 data to x and y = 150-x\n",
    "\n",
    "    dataX = data[['sepal_length','sepal_width','petal_length','petal_width']] # taking the training data features\n",
    "    dataY = data.species # output of our training data\n",
    "    testX = test[['sepal_length','sepal_width','petal_length','petal_width']] # taking test data features\n",
    "    testY = test.species   # output value of test data\n",
    "\n",
    "    dataX = dataX.reset_index(drop=True) \n",
    "    dataY = dataY.reset_index(drop=True)\n",
    "    testX = testX.reset_index(drop=True)\n",
    "    testY = testY.reset_index(drop=True)\n",
    "\n",
    "    #k-fold\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(data)\n",
    "    indices = kf.split(data)\n",
    "\n",
    "    mainc45 = None\n",
    "    maxacc = 0;\n",
    "\n",
    "    for train_index, test_index in indices: \n",
    "        train = data.iloc[train_index]\n",
    "        train = train.reset_index(drop=True)\n",
    "\n",
    "        valX, valY = dataX.iloc[test_index], dataY.iloc[test_index]\n",
    "        valX = valX.reset_index(drop=True)\n",
    "        valY = valY.reset_index(drop=True)\n",
    "\n",
    "        attributes = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "        target = 'species'\n",
    "        c45 = myC45(train, target, attributes)\n",
    "        prediction = []\n",
    "        for i in range(len(valX)):\n",
    "            prediction.append(c45.classify(c45.id3.tree_, valX.iloc[i]))\n",
    "        if (accuracy_score(valY.values, prediction) > maxacc):\n",
    "            mainc45 = c45\n",
    "            maxacc = accuracy_score(valY.values, prediction)\n",
    "    finalPrediction = []\n",
    "    for i in range(len(testX)):\n",
    "        finalPrediction.append(mainc45.classify(mainc45.id3.tree_, testX.iloc[i]))\n",
    "    print(\"Accuracy Score: \")\n",
    "    print(accuracy_score(testY.values, finalPrediction))\n",
    "    # experiment(mainc45)\n",
    "\n",
    "\n",
    "split()\n",
    "kfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vk7EAPpBhiTg"
   },
   "source": [
    "# **Analisis**\n",
    "\n",
    "## **File eksternal**\n",
    "\n",
    "Sebelum setiap metode (confusion matrix maupun k-fold) dilakukan, DTL maupun MLP men-save dan men-load file ke dalam file eksternal.\n",
    "\n",
    "\n",
    "**Model DTL**\n",
    "\n",
    "Model DTL (C45) disave dalam bentuk tree.  Model DTL diload dengan cara membaca setiap spasi dari file eksternal, lalu jumlah spasi tersebut akan dijadikan referensi apakah dia nama atribut atau value dari nama atribut tersebut. Selain itu, jumlah spasi yang semakin dalam menandakan bahwa nama atribut / value dari nama atribut tersebut adalah milik pohon generasi yang lebih muda (contoh: spasi 1 = parent, spasi 2 = anak).\n",
    "\n",
    "**Model ANN**\n",
    "\n",
    "Model ANN (MLP) di save ke dalam file eksternal berdasarkan jumlah layer, weight, dan perceptron pada setiap layer. Model MLP di-load dari file eksternal berdasarkan lokasi dari layer, weight, dan perceptron. Setiap line baru pada file eksternal menandakan layer baru. Setiap weight akan dibatasi dengan \"p\" pada file eksternal. Sebelum terbaca huruf \"p\", weight adalah milik satu perceptron.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "-5.31792429650554 5.08384530165187 -2.2912199946989236 p \n",
    "\n",
    "Maka:\n",
    "\n",
    "weight 1 = -5.31792429650554, weight 2 = 5.08384530165187, weight 3 = -2.2912199946989236 milik perceptron 1.\n",
    "\n",
    "### Notes About ANN\n",
    "\n",
    "Pada kasus ANN (MLP), ANN(MLP) dilatih dengan hidden layer 3 buah dengan masing-masing: 10 perceptron, 5 perceptron, dan 2 perceptron. Learning rate yang digunakan adalah 0,05 dan Iterasi maksimumnya adalah 400. Diberikan batch_size = 32 dan treshold eror = 10^-4.\n",
    "\n",
    "\n",
    "## **Train-Split Test**\n",
    "\n",
    "Pada DTL dan ANN (MLP), kami menggunakan data split yaitu 90-10, dan kami split menggunakan train_test_split yang tersedia di library `sklearn.model_selection`. Artinya, 90% digunakan sebagai data training dan 10% digunakan sebagai data testing. Pemilihan data training dan data testing dilakukan secara random!\n",
    "\n",
    "Diperlukan Confusion Matrix pada Train-Split test. Untuk mengimplementasi Confusion Matrix, menghitung akurasi, dan membuat Classification Report, kami menggunakan library `sklearn.metrics`.\n",
    "\n",
    "Pada Classification Report, terdapat:<br>\n",
    "Precision: ratio true positive dari seluruh hasil positive yang didapatkan (tp / (tp + fp)) di mana tp merupakan angka kasus true positive dan fp merupakan angka kasus false positive <br>\n",
    "Recall: ratio true positive dari hasil positive yang seharusnya ada (tp / (tp + fn)) di mana tp merupakan angka kasus true positive dan fn merupakan angka kasus false negative <br>\n",
    "F1-score: Harmonic mean dari Precision dan Recall <br>\n",
    "Support: jumlah kemunculan setiap kelas di hasil sebenarnya <br>\n",
    "\n",
    "Pada kasus MLP, Didapatkan akurasi 93.33%. Artinya hasil prediksi menggunakan MLP 93.33% benar terhadap hasil seharusnya.\n",
    "\n",
    "Pada kasus DTL, Didapatkan akurasi 66.66%. Artinya hasil prediksi menggunakan DTL 66.66% benar terhadap hasil seharusnya.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Analisis**\n",
    "\n",
    "## **File eksternal**\n",
    "\n",
    "Sebelum setiap metode (confusion matrix maupun k-fold) dilakukan, DTL maupun MLP men-save dan men-load file ke dalam file eksternal.\n",
    "\n",
    "\n",
    "**Model DTL**\n",
    "\n",
    "Model DTL (C45) disave dalam bentuk tree.  Model DTL diload dengan cara membaca setiap spasi dari file eksternal, lalu jumlah spasi tersebut akan dijadikan referensi apakah dia nama atribut atau value dari nama atribut tersebut. Selain itu, jumlah spasi yang semakin dalam menandakan bahwa nama atribut / value dari nama atribut tersebut adalah milik pohon generasi yang lebih muda (contoh: spasi 1 = parent, spasi 2 = anak).\n",
    "\n",
    "**Model ANN**\n",
    "\n",
    "Model ANN (MLP) di save ke dalam file eksternal berdasarkan jumlah layer, weight, dan perceptron pada setiap layer. Model MLP di-load dari file eksternal berdasarkan lokasi dari layer, weight, dan perceptron. Setiap line baru pada file eksternal menandakan layer baru. Setiap weight akan dibatasi dengan \"p\" pada file eksternal. Sebelum terbaca huruf \"p\", weight adalah milik satu perceptron.\n",
    "\n",
    "Contoh:\n",
    "\n",
    "-5.31792429650554 5.08384530165187 -2.2912199946989236 p \n",
    "\n",
    "Maka:\n",
    "\n",
    "weight 1 = -5.31792429650554, weight 2 = 5.08384530165187, weight 3 = -2.2912199946989236 milik perceptron 1.\n",
    "\n",
    "### Notes About ANN\n",
    "\n",
    "Pada kasus ANN (MLP), ANN(MLP) dilatih dengan hidden layer 3 buah dengan masing-masing: 10 perceptron, 5 perceptron, dan 2 perceptron. Learning rate yang digunakan adalah 0,05 dan Iterasi maksimumnya adalah 400. Diberikan batch_size = 32 dan treshold eror = 10^-4.\n",
    "\n",
    "### Terminologi\n",
    "Pada Classification Report, terdapat:<br>\n",
    "Precision: ratio true positive dari seluruh hasil positive yang didapatkan (tp / (tp + fp)) di mana tp merupakan angka kasus true positive dan fp merupakan angka kasus false positive <br>\n",
    "Recall: ratio true positive dari hasil positive yang seharusnya ada (tp / (tp + fn)) di mana tp merupakan angka kasus true positive dan fn merupakan angka kasus false negative <br>\n",
    "F1-score: Harmonic mean dari Precision dan Recall <br>\n",
    "Support: jumlah kemunculan setiap kelas di hasil sebenarnya <br>\n",
    "\n",
    "\n",
    "## **Train-Split Test**\n",
    "\n",
    "Pada DTL dan ANN (MLP), kami menggunakan data split yaitu 90-10, dan kami split menggunakan train_test_split yang tersedia di library `sklearn.model_selection`. Artinya, 90% digunakan sebagai data training dan 10% digunakan sebagai data testing. Pemilihan data training dan data testing dilakukan secara random!\n",
    "\n",
    "Diperlukan Confusion Matrix pada Train-Split test. Untuk mengimplementasi Confusion Matrix, menghitung akurasi, dan membuat Classification Report, kami menggunakan library `sklearn.metrics`.\n",
    "\n",
    "### Kasus ANN\n",
    "Pada kasus ANN, Didapatkan akurasi `93,3333%`. Perhatikan bahwa 93.3333% adalah hasil yang diuji kepada data testing.\n",
    "Confusion Matrix yang didapat adalah:\n",
    "\n",
    "[[3 0 0] \n",
    "[ 0 4 1]\n",
    "[ 0 0 7]]\n",
    "\n",
    "*Kelas 1 = Iris-setosa\n",
    "*Kelas 2 = Iris-versicolor\n",
    "*Kelas 3 = Iris-virginica\n",
    "\n",
    "Artinya, 3 instance berhasil diklasifikasi di kelas 1 (true kelas 1), 4 instance berhasil diklasifikasi di kelas 2 (true kelas 2), namun terdapat 1 instance yang seharusnya berada di kelas 3 namun diklasifikasikan di kelas 2. Kemudian, terdapat 7 instance yang berhasil diklasifikasi di kelas 3 (true kelas 3).\n",
    "\n",
    "Sesuai dengan terminologi, precision dari kelas Iris-setosa adalah 1, Iris-versicolor adalah 1, dan Iris-virginica adalah 0.88. Perhatikan bahwa 0.88 didapat dari `7 instance / (7 instance benar + 1 instance seharusnya kelas 3 namun terdeteksi di kelas 2)`.\n",
    "\n",
    "Recall dari kelas Iris-setosa adalah 1, Iris-versicolor adalah 0.8, dan Iris-virginica adalah 1. Perhatikan bahwa 0.8 didapat dari 4 instance / (4 instance benar + 1 instance seharusnya bukan kelas 2)\n",
    "\n",
    "f1-score dari kelas Iris-virginica adalah 1, Iris-versicolor adalah 0.89, dan Iris-virginica adalah 0.93. f1-score didapat dari rata-rata harmonik antara presisi dan recall.\n",
    "\n",
    "Support adalah jumlah kemunculan instance pada hasil sebenarnya: Iris-setosa = 3, Iris-versicolor = 5, Iris-virginica = 7.\n",
    "\n",
    "### Kasus DTL\n",
    "Pada kasus DTL, Didapatkan akurasi `66.666%`. Perhatikan bahwa 66.666% adalah hasil yang diuji kepada data testing.\n",
    "Confusion Matrix yang didapat adalah:\n",
    "\n",
    "[[3 0 0] \n",
    "[ 0 4 5]\n",
    "[ 0 0 3]]\n",
    "\n",
    "*Kelas 1 = Iris-setosa\n",
    "*Kelas 2 = Iris-versicolor\n",
    "*Kelas 3 = Iris-virginica\n",
    "\n",
    "Artinya, 3 instance berhasil diklasifikasi di kelas 1 (true kelas 1), 4 instance berhasil diklasifikasi di kelas 2 (true kelas 2), namun terdapat 5 instance yang seharusnya berada di kelas 3 namun diklasifikasikan di kelas 2. Kemudian, terdapat 3 instance yang berhasil diklasifikasi di kelas 3 (true kelas 3).\n",
    "\n",
    "Sesuai dengan terminologi, precision dari kelas Iris-setosa adalah 1, Iris-versicolor adalah 1, dan Iris-virginica adalah 0.38.  0.38 didapat dari `3 instance benar / 3 instance benar + 5 instance yang seharusnya diklasifikasikan di kelas 1`.\n",
    "\n",
    "Recall dari kelas Iris-setosa adalah 1, Iris-versicolor adalah 0.44, dan Iris-virginica adalah 1. Perhatikan bahwa 0.44 didapat dari `4 instance benar / (4 instance benar + 5 instance seharusnya bukan kelas 2)`.\n",
    "\n",
    "f1-score dari kelas Iris-virginica adalah 1, Iris-versicolor adalah 0.62, dan Iris-virginica adalah 0.55. f1-score didapat dari rata-rata harmonik antara presisi dan recall.\n",
    "\n",
    "Support adalah jumlah kemunculan instance pada hasil sebenarnya: Iris-setosa = 3, Iris-versicolor = 9, Iris-virginica = 3.\n",
    "\n",
    "## **Kfold**\n",
    "\n",
    "Untuk mengimplementasi 10-fold, kami menggunakan library sklearn.model_selection. Dari seluruh data yang ada, kami mengambil split 80-20. Artinya, 80% digunakan sebagai data training dan 20% digunakan sebagai data testing, dengan pemilihan data training dan data testing dilakukan secara random. Lalu, dari 80% data training tersebut pada kfold, akan dilakukan pembagian, dengan setiap pembagian akan mengandung 1 data validasi dan 9 data training. Lalu akan dicek setiap pembagian tersebut dan akan dibandingkan dengan data training baru, lalu mengambil pembagian yang memiliki akurasi prediksi yang paling tinggi.\n",
    "\n",
    "Pada kasus MLP, Didapatkan akurasi 96.67%. Artinya hasil prediksi menggunakan MLP 96.67% benar terhadap hasil seharusnya.\n",
    "\n",
    "Pada kasus DTL, Didapatkan akurasi 63.33%. Artinya hasil prediksi menggunakan DTL 63.33% benar terhadap hasil seharusnya.\n",
    "\n",
    "\n",
    "## **Kesimpulan**\n",
    "\n",
    "Menurut hasil yang sudah didapatkan, ANN menggunakan myMLP lebih bagus dibandingkan DTL karena akurasinya lebih tinggi. Alasannya adalah, jumlah hidden layer yang tepat (3 buah), iterasi sebanyak 400, dan Xavier Initialization untuk menentukan weight awal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSj6OS5PTMMB"
   },
   "outputs": [],
   "source": [
    "\"/\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tubes1D_13517044.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
